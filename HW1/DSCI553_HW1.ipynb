{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXEhN73PGn46",
        "outputId": "d6705e13-4c72-476d-d015-d5223eff9115"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=457e0b5cbcbf9e9f1664be86c09519bb79070afb168482ae4645e07c0418984a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yHW79KYGGez_"
      },
      "outputs": [],
      "source": [
        "\"\"\"Task1.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1HMePKJKMnucU_oP2Cjg30P05zUszynMf\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pyspark import SparkContext\n",
        "\n",
        "\n",
        "input_path = sys.argv[1]\n",
        "output_path = sys.argv[2]\n",
        "#input_path = \"test_review.json\"\n",
        "#output_path = \"output.json\"\n",
        "\n",
        "\n",
        "\n",
        "sc = SparkContext('local[*]', 'hw1_task1')\n",
        "reviews_rdd = sc.textFile(input_path).map(lambda line: json.loads(line))\n",
        "\n",
        "#A\n",
        "total_reviews = reviews_rdd.count()\n",
        "#B\n",
        "reviews_2018 = reviews_rdd.filter(lambda line: line['date'][:4] == '2018').count()\n",
        "#C\n",
        "distinct_users = reviews_rdd.map(lambda line: line['user_id']).distinct().count()\n",
        "#D\n",
        "top10_users = reviews_rdd.map(lambda line: (line['user_id'], 1)).reduceByKey(lambda x, y: x+y).sortBy(lambda x: (-x[1], x[0])).take(10)\n",
        "#E\n",
        "distinct_business = reviews_rdd.map(lambda line: line['business_id']).distinct().count()\n",
        "#F\n",
        "top10_business = reviews_rdd.map(lambda line: (line['business_id'], 1)).reduceByKey(lambda x, y: x+y).sortBy(lambda x: (-x[1], x[0])).take(10)\n",
        "\n",
        "output = {'n_review': total_reviews,\n",
        "          'n_review_2018': reviews_2018,\n",
        "          'n_user': distinct_users,\n",
        "          'top10_user': top10_users,\n",
        "          'n_business': distinct_business,\n",
        "          'top10_business': top10_business}\n",
        "\n",
        "\n",
        "with open(output_path, 'w') as output_file:\n",
        "    json.dump(output, output_file)\n",
        "\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#=========== TASK 2 =================="
      ],
      "metadata": {
        "id": "i69xDzKlNkCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"task2.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1XdKzKxMnXxOnNB6iS7Juode3PIItDnTa\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pyspark import SparkContext\n",
        "import random\n",
        "import time\n",
        "\n",
        "\n",
        "input_path = sys.argv[1]\n",
        "output_path = sys.argv[2]\n",
        "n_partition = sys.argv[3]\n",
        "#input_path = \"test_review.json\"\n",
        "#output_path = \"output2.json\"\n",
        "#n_partition = 2\n",
        "\n",
        "\n",
        "sc = SparkContext('local[*]', 'hw1_task2')\n",
        "reviews_rdd = sc.textFile(input_path).map(lambda line: json.loads(line))\n",
        "\n",
        "result = {}\n",
        "result[\"default\"] = {}\n",
        "result[\"customized\"] = {}\n",
        "\n",
        "def get_partition(x):\n",
        "    return (len(x[0]) + random.randint(0, 1000)) % int(n_partition)\n",
        "\n",
        "#default\n",
        "default_start = time.time()\n",
        "default_rdd = reviews_rdd.map(lambda row: (row['business_id'], 1))\n",
        "default_partitions = default_rdd.getNumPartitions()\n",
        "default_n_items = default_rdd.mapPartitions(lambda iterator: [len(list(iterator))], True).collect()\n",
        "\n",
        "d_temp = default_rdd.reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], ascending=False).map(lambda line: [line[0], line[1]]).take(10)\n",
        "default_exe_time = time.time() - default_start\n",
        "\n",
        "#Customized\n",
        "custom_start = time.time()\n",
        "customized_rdd = reviews_rdd.map(lambda x: (x['business_id'], 1)).partitionBy(int(n_partition), lambda x: ord(x[0][0]) % int(n_partition)).cache()\n",
        "customized_partitions = customized_rdd.getNumPartitions()\n",
        "customized_n_items = customized_rdd.mapPartitions(lambda iterator: [len(list(iterator))], True).collect()\n",
        "c_temp = customized_rdd.reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], ascending=False).map(lambda line: [line[0], line[1]]).take(10)\n",
        "customized_exe_time = time.time() - custom_start\n",
        "\n",
        "\n",
        "output = {'default': {'n_partition': default_partitions,\n",
        "                      'n_items': default_n_items,\n",
        "                      'exe_time': default_exe_time},\n",
        "          'customized': {'n_partition': customized_partitions,\n",
        "                         'n_items': customized_n_items,\n",
        "                         'exe_time': customized_exe_time}}\n",
        "\n",
        "with open(output_path, 'w') as output_file:\n",
        "      json.dump(output, output_file)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "8gPluNGiNsja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#=========== TASK 3 =================="
      ],
      "metadata": {
        "id": "MgW8GogENxkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"task3.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/18_I3iYXPb0MDSEy-R_qprzXwC0vUJwHb\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "from pyspark import SparkContext\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "review_path = sys.argv[1]\n",
        "business_path = sys.argv[2]\n",
        "output_path_a = sys.argv[3]\n",
        "output_path_b = sys.argv[4]\n",
        "#review_path = \"test_review.json\"\n",
        "#business_path = \"business.json\"\n",
        "#output_path_a = \"output3a.txt\"\n",
        "#output_path_b = \"output3b.json\"\n",
        "\n",
        "\n",
        "\n",
        "sc = SparkContext('local[*]', 'task3')\n",
        "\n",
        "\n",
        "review_rdd = sc.textFile(review_path).map(lambda line: json.loads(line))\n",
        "business_rdd = sc.textFile(business_path).map(lambda line: json.loads(line))\n",
        "\n",
        "review = sc.textFile(review_path).map(json.loads).map(lambda x: (x['business_id'], x['stars']))\n",
        "business = sc.textFile(business_path).map(json.loads).map(lambda x: (x['business_id'], x['city']))\n",
        "\n",
        "data = review.join(business).map(lambda x: (x[1][1], x[1][0])).groupByKey().map(lambda x: (x[0], sum(x[1]) / len(x[1])))\n",
        "\n",
        "\n",
        "data_temp = (\n",
        "    review_rdd\n",
        "    .map(lambda review: (review[\"business_id\"], review[\"stars\"]))\n",
        "    .join(business_rdd.map(lambda business: (business[\"business_id\"], business[\"city\"])))\n",
        "    .values()  # Get the (stars, city) pairs\n",
        "    .groupByKey()\n",
        "    .mapValues(lambda stars: sum(stars) / len(stars))  # Calculate average stars for each city\n",
        ")\n",
        "\n",
        "#data_a = sorted(data.collect(), key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "\n",
        "m1_start = time.time()\n",
        "data_2 = data.collect()\n",
        "data_2.sort(key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "print([s for s in data_2[:10]])\n",
        "m1_time = time.time() - m1_start\n",
        "\n",
        "with open(output_path_a, \"w\") as output_file_a:\n",
        "  output_file_a.write(\"city,stars\\n\")\n",
        "  #for city, stars in data.sortBy(lambda x: (-x[1], x[0])).collect():\n",
        "        #output_file_a.write(f\"{city},{stars}\\n\")\n",
        "  for line in data_2:\n",
        "    output_file_a.write(line[0]+\",\"+str(line[1])+\"\\n\")\n",
        "\n",
        "m2_start = time.time()\n",
        "print(data.takeOrdered(10, key=lambda x: (-x[1], x[0])))\n",
        "m2_time = time.time() - m2_start\n",
        "\n",
        "output = {'m1': m1_time,\n",
        "          'm2': m2_time,\n",
        "          'reason': \"Sorting data in python returns a part of the RDD, whereas using spark the entire RDD is returened which is a more expensive operation\"\n",
        "}\n",
        "\n",
        "with open(output_path_b, 'w') as output_file_b:\n",
        "  json.dump(output, output_file_b)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "ZmVaTU8UN0wl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}