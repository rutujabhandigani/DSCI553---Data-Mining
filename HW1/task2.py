# -*- coding: utf-8 -*-
"""Task2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nZvy1zVZ8-r2lA5hNsToYTLQgp-FnncB
"""

import os
import sys
import json
from pyspark import SparkContext
import random
import time


#input_path = sys.argv[1]
#output_path = sys.argv[2]
#n_partition = sys.argv[3]
input_path = "test_review.json"
output_path = "output2.json"
n_partition = 2

#os.environ['PYSPARK_PYTHON'] = '/usr/local/bin/python3.6'
#os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/local/bin/python3.6'

sc = SparkContext('local[*]', 'hw1_task2')
reviews_rdd = sc.textFile(input_path).map(lambda line: json.loads(line))

output = {}
output["default"] = {}
output["customized"] = {}

def get_partition(x):
    return (len(x[0]) + random.randint(0, 1000)) % int(n_partition)

#default
default_start = time.time()
default_rdd = reviews_rdd.map(lambda row: (row['business_id'], 1))
output["default"]["n_partition"] = default_rdd.getNumPartitions()
output["default"]["n_items"] = default_rdd.mapPartitions(lambda iterator: [len(list(iterator))], True).collect()

d_temp = default_rdd.reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], ascending=False).map(lambda line: [line[0], line[1]]).take(10)
output["default"]["exe_time"] = time.time() - default_start

#Customized
custom_start = time.time()
customized_rdd = reviews_rdd.map(lambda x: (x['business_id'], 1)).partitionBy(int(n_partition), lambda x: ord(x[0][0]) % int(n_partition)).cache()
output["customized"]["n_partition"] = customized_rdd.getNumPartitions()
output["customized"]["n_items"] = customized_rdd.mapPartitions(lambda iterator: [len(list(iterator))], True).collect()
c_temp = customized_rdd.reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], ascending=False).map(lambda line: [line[0], line[1]]).take(10)
output["customized"]["exe_time"] = time.time() - custom_start


with open(output_path, 'w') as output_file:
      json.dump(output, output_file)

sc.stop()