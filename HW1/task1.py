# -*- coding: utf-8 -*-
"""Task1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HMePKJKMnucU_oP2Cjg30P05zUszynMf
"""

import os
import sys
import json
from pyspark import SparkContext


input_path = sys.argv[1]
output_path = sys.argv[2]
#input_path = "test_review.json"
#output_path = "output.json"

os.environ['PYSPARK_PYTHON'] = '/usr/local/bin/python3.6'
os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/local/bin/python3.6'


sc = SparkContext('local[*]', 'hw1_task1')
reviews_rdd = sc.textFile(input_path).map(lambda line: json.loads(line))

output = {}

output['n_review'] = reviews_rdd.count()

output['n_review_2018'] = reviews_rdd.filter(lambda line: line['date'][:4] == '2018').count()

output['n_user'] = reviews_rdd.map(lambda line: line['user_id']).distinct().count()

output['top10_user'] = reviews_rdd.map(lambda line: (line['user_id'], 1)).reduceByKey(lambda x, y: x+y).sortBy(lambda x: (-x[1], x[0])).take(10)

output['n_business'] = reviews_rdd.map(lambda line: line['business_id']).distinct().count()

output['top10_business'] = reviews_rdd.map(lambda line: (line['business_id'], 1)).reduceByKey(lambda x, y: x+y).sortBy(lambda x: (-x[1], x[0])).take(10)

with open(output_path, 'w') as output_file:
    json.dump(output, output_file)

sc.stop()