# -*- coding: utf-8 -*-
"""task2_rb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rz4lLQ_g-2i8sdDU7UyDOn6D8LF_H4wd
"""

from pyspark import SparkContext
import time
import sys
import math
import os
import itertools


def get_baskets(line, case_n):
    l = line.split(',')
    if case_n > 2:
        return None, None
    elif case_n == 1:
        return l[0], l[1]
    elif case_n == 2:
        return l[1], l[0]

def preprocess(input_path, intermediate):
    lines = []
    temp_file = open(input_path, "r", encoding='utf-8')
    for line in temp_file.readlines()[1:]:
        line = line.strip().split(',')
        cust_year = line[0][:-4] + line[0][-2:] + "-" + line[1].lstrip('0')
        lines.append([cust_year.replace('"', ''), line[5].replace('"', '').lstrip("0")])
    with open(intermediate, 'w') as temp_f:
        temp_f.write("DATE-CUSTOMER_ID,PRODUCT_ID\n")
        for l in lines:
            temp_f.write(",".join(l) + "\n")
    return


def flat_set(x):
    if case_n == 2:
        return x
    else:
        new_basket = set()
        print(x)
        for xx in x[1]:
            print(xx)
        return x[0], new_basket

def get_frequent(partition, candidate_rdd):
    frequency = {}
    for item in partition:
        for c in candidate_rdd:
            if set(item).issuperset(set(c)):
                frequency[c] = frequency.get(c, 0) + 1
    output = []
    for key, value in frequency.items():
        output.append((key, value))
    return output


def apriori(partition, support, basket_count):
    items = {}
    chunk = list(partition)
    support_chunk = math.ceil(len(chunk) * support / basket_count)

    for basket in chunk:
        for item in basket:
            item = (item,)
            items[item] = items.get(item, 0) + 1

    candidates = []
    for item, count in items.items():
        if count >= support_chunk:
            candidates.append(item)

    can_len = len(candidates)
    output = []
    if can_len > 0:
        output.append(candidates)
    else:
        return output
    candidates = set(x[0] for x in candidates)
    idx = 2
    while True:
        items = {}
        for basket in chunk:
            common = sorted(list(set(basket).intersection(candidates)))
            for item in itertools.combinations(common, idx):
                items[item] = items.get(item, 0) + 1

        candidates = []
        for item, count in items.items():
            if count >= support_chunk:
                  candidates.append(item)

        can_len = len(candidates)

        if can_len > 0:
            output.append(candidates)
        else:
            break
        candidates = set([item for basket in candidates for item in basket])
        idx = idx + 1
    return output


if __name__ == '__main__':

    #threshold = 20
    #support = 50
    #input_path = "ta_feng_all_months_merged.csv"
    #output_path = "task2_output_rf1.txt"

    intermediate = "customer_product.csv"
    threshold = int(sys.argv[1])
    support = int(sys.argv[2])
    input_path = sys.argv[3]
    output_path = sys.argv[4]

    
    preprocess(input_path, intermediate)

    sc = SparkContext('local[*]', 'task2')
    sc.setLogLevel("ERROR")

    os.environ['PYSPARK_PYTHON'] = '/usr/local/bin/python3.6'
    os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/local/bin/python3.6'

    start = time.time()

    rdd = sc.textFile(intermediate)
    header = rdd.first()
    data = rdd.filter(lambda r: r != header).map(lambda row: row.strip().split(","))

    result = ""
    res_can = ""
    res_freq = ""

    baskets = data.map(lambda row: (str(row[0]), str(row[1]))).groupByKey().mapValues(list).filter(lambda row: len(row[1]) > threshold).map(lambda row: sorted(list(set(row[1]))))
    baskets_count = baskets.count()

    candidate_rdd = baskets.mapPartitions(lambda partition: apriori(partition, support, baskets_count)).flatMap(lambda basket: [x for x in basket])
    candidate_rdd = candidate_rdd.distinct().sortBy(lambda x: (len(x), x)).collect()
    length = 1
    for item in list(candidate_rdd):
        if len(item) != length:
            length = length + 1
            res_can = res_can[:-1]
            res_can = res_can + "\n\n"
        if len(item) == 1:
            res_can = res_can + str(item).replace(",", "") + ","
        else:
            res_can = res_can + str(item) + ","
    res_can = res_can[:-1]

    frequent_rdd = baskets.mapPartitions(lambda partition: get_frequent(partition, candidate_rdd))
    frequent_rdd = frequent_rdd.reduceByKey(lambda a, b: a + b).filter(lambda x: x[1] >= support).map(lambda x: x[0])
    frequent_rdd = frequent_rdd.sortBy(lambda x: (len(x), x)).collect()

    length = 1
    for item in list(frequent_rdd):
        if len(item) != length:
            length = length + 1
            res_freq = res_freq[:-1]
            res_freq = res_freq + "\n\n"
        if len(item) == 1:
            res_freq = res_freq + str(item).replace(",", "") + ","
        else:
            res_freq = res_freq + str(item) + ","
    res_freq = res_freq[:-1]

    result = result + "Candidates:\n" + res_can + "\nFrequent Itemsets:\n" + res_freq
    with open(output_path, 'w') as out_file:
          out_file.writelines(result)

    duration = time.time() - start
    print("Duration:", duration)

    sc.stop()