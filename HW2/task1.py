# -*- coding: utf-8 -*-
"""task1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jO2VSvJpJLF0pqO0nErIBTI_xfQpFR0l
"""

from pyspark import SparkContext
import time
import sys
import math
import os
import itertools


def get_baskets(line, case_n):
    l = line.split(',')
    if case_n > 2:
        return None, None
    elif case_n == 1:
        return l[0], l[1]
    elif case_n == 2:
        return l[1], l[0]


def flat_set(x):
    if case_n == 2:
        return x
    else:
        new_basket = set()
        print(x)
        for xx in x[1]:
            print(xx)
        return x[0], new_basket

def get_frequent(partition, candidate_rdd):
    frequency = {}
    for item in partition:
        for c in candidate_rdd:
            if set(item).issuperset(set(c)):
                frequency[c] = frequency.get(c, 0) + 1
    output = []
    for key, value in frequency.items():
        output.append((key, value))
    return output


def apriori(partition, support, basket_count):
    items = {}
    chunk = list(partition)
    support_chunk = math.ceil(len(chunk) * support / basket_count)

    for basket in chunk:
        for item in basket:
            item = (item,)
            items[item] = items.get(item, 0) + 1

    candidates = []
    for item, count in items.items():
        if count >= support_chunk:
            candidates.append(item)

    can_len = len(candidates)
    output = []
    if can_len > 0:
        output.append(candidates)
    else:
        return output
    candidates = set(x[0] for x in candidates)
    idx = 2
    while True:
        items = {}
        for basket in chunk:
            common = sorted(list(set(basket).intersection(candidates)))
            for item in itertools.combinations(common, idx):
                items[item] = items.get(item, 0) + 1

        candidates = []
        for item, count in items.items():
            if count >= support_chunk:
                  candidates.append(item)

        can_len = len(candidates)

        if can_len > 0:
            output.append(candidates)
        else:
            break
        candidates = set([item for basket in candidates for item in basket])
        idx = idx + 1
    return output


if __name__ == '__main__':
    #case_n = 2
    #support = 9
    #input_path = "small2.csv"
    #output_path = "task1_output_rf18.txt"

    case_n = int(sys.argv[1])
    support = int(sys.argv[2])
    input_path = sys.argv[3]
    output_path = sys.argv[4]
    
    sc = SparkContext('local[*]', 'task1')
    sc.setLogLevel("ERROR")

    os.environ['PYSPARK_PYTHON'] = '/usr/local/bin/python3.6'
    os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/local/bin/python3.6'

    start = time.time()
    rdd = sc.textFile(input_path)
    header = rdd.first()
    data = rdd.filter(lambda r: r != header)

    result = ""
    res_can = ""
    res_freq = ""

    baskets = data.map(lambda line: get_baskets(line, case_n)).groupByKey().mapValues(set).map(lambda x: x[1]).persist()
    baskets_count = baskets.count()

    candidate_rdd = baskets.mapPartitions(lambda partition: apriori(partition, support, baskets_count)).flatMap(lambda basket: [x for x in basket])
    candidate_rdd = candidate_rdd.distinct().sortBy(lambda x: (len(x), x)).collect()
    
    
    length = 1
    for item in list(candidate_rdd):
        if len(item) != length:
            length = length + 1
            res_can = res_can[:-1]
            res_can = res_can + "\n\n"
        if len(item) == 1:
            res_can = res_can + str(item).replace(",", "") + ","
        else:
            res_can = res_can + str(item) + ","
    res_can = res_can[:-1]

    frequent_rdd = baskets.mapPartitions(lambda partition: get_frequent(partition, candidate_rdd))
    frequent_rdd = frequent_rdd.reduceByKey(lambda a, b: a + b).filter(lambda x: x[1] >= support).map(lambda x: x[0])
    frequent_rdd = frequent_rdd.sortBy(lambda x: (len(x), x)).collect()
     

    length = 1
    for item in list(frequent_rdd):
        if len(item) != length:
            length = length + 1
            res_freq = res_freq[:-1]
            res_freq = res_freq + "\n\n"
        if len(item) == 1:
            res_freq = res_freq + str(item).replace(",", "") + ","
        else:
            res_freq = res_freq + str(item) + ","
    res_freq = res_freq[:-1]

    
    result = result + "Candidates:\n" + res_can + "\nFrequent Itemsets:\n" + res_freq
    with open(output_path, 'w') as out_file:
          out_file.writelines(result)

    duration = time.time() - start
    print("Duration:", duration)

    sc.stop()